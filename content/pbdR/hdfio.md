# Parallel I/O with hdfio


## Background

The [hdfio package](https://github.com/RBigData/hdfio) offers a set of high-level utilities for working with [HDF5](https://www.hdfgroup.org/). It defines a dataframe format in hdf5 storage and offers simple readers and converters for working with that format. It does not expose the full capabilities of HDF5. For that, see the [rhdf5](https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html) and [hdf5r](https://cran.r-project.org/web/packages/hdf5r/index.html) packages (we actually use hdf5r internally).

With the package, you can easily convert CSVs into dataframes stored in binary hdf5 files. This can be done in batches for very large CSV files (so you need not read the entire dataset into memory), and can also convert directories of CSVs into a single hdf5 file. For the converters, see `hdfio::csv2h5()` and `hdfio::dir2h5()`.

The [hdfio package README](https://github.com/RBigData/hdfio/blob/master/README.md) contains a lot of useful demonstrations of how to use these functions. Among them is an example that constructs a single in-hdf5 dataframe of the entirety of the famous [airlines dataset](http://stat-computing.org/dataexpo/2009/). In this demonstration, we will routinely refer to the hdf5 file `airlines.h5`, which is the output generated by example in the hdfio README.



## MPI Basics

With the data in the "h5df" format native to the hdfio package, it is very easy to read subset of rows and/or columns:

```r
hdfio::read_h5df("airlines.h5", rows=1:5, cols=1:5)
##   Year Month DayofMonth DayOfWeek DepTime
## 1 1987    10         14         3     741
## 2 1987    10         15         4     729
## 3 1987    10         17         6     741
## 4 1987    10         18         7     729
## 5 1987    10         19         1     749
```

We can quickly build a parallel reader by simply supplying different subsets of rows to different MPI processes. We will be using the `get.jid()` function from the [pbdMPI package](https://cran.r-project.org/web/packages/pbdMPI/index.html) to select the rows each rank will read (you could also use the `comm.chunk()` from the [pbdIO package](https://github.com/RBigData/pbdIO)). The function will divide `n` tasks as evenly as possible across the MPI ranks. In this way, we can change our resource size (number of MPI ranks) without needing to change our script.

The `get.jid()` function is discussed in the [pbdMPI tutorial](mpi.md). There you can find more information and some examples demonstrating how it works.



## Reading

With `hdfio::read_h5df()` and `pbdMPI::get.jid()`, we have all the pieces necessary to put everything together. We use `get.jid()` to select the rows for each MPI process. For the sake of demonstration, we will read only the first 1000 rows and only use 2 MPI ranks:

```r
suppressMessages(library(pbdMPI))
suppressMessages(library(hdfio))

f = "airlines.h5"
nrows = 1000
rows = get.jid(nrows)

x = read_h5df(f, rows=rows, cols=1:5)
comm.print(str(x), all.rank=TRUE)

finalize()
```

If we can save this file as `read.r` and run it with 2 ranks then we see:

```bash
$ mpirun -np 2 Rscript read.r
## 'data.frame':	500 obs. of  5 variables:
##  $ Year      : int  1987 1987 1987 1987 1987 1987 1987 1987 1987 1987 ...
##  $ Month     : int  10 10 10 10 10 10 10 10 10 10 ...
##  $ DayofMonth: int  14 15 17 18 19 21 22 23 24 25 ...
##  $ DayOfWeek : int  3 4 6 7 1 3 4 5 6 7 ...
##  $ DepTime   : int  741 729 741 729 749 728 728 731 744 729 ...
## NULL
## 'data.frame':	500 obs. of  5 variables:
##  $ Year      : int  1987 1987 1987 1987 1987 1987 1987 1987 1987 1987 ...
##  $ Month     : int  10 10 10 10 10 10 10 10 10 10 ...
##  $ DayofMonth: int  5 6 7 8 11 12 13 14 15 16 ...
##  $ DayOfWeek : int  1 2 3 4 7 1 2 3 4 5 ...
##  $ DepTime   : int  NA 1902 1850 1941 1853 1859 1942 1914 1858 1849 ...
## NULL
```

Here the data is being read "by row", meaning that rows are contiguous on the MPI ranks. Said another way, if a process owns any part of a row, then it owns the entire row. More complex things are possible, but this is a very good start. Some more complicated strategies include reading directly into a block-cyclic format, or creating a new MPI communicator just for reading.

Once the data has been read in parallel, you can use our distributed matrix packages to compute on the data. See our distributed matrix tutorials for examples.



**Recap**:

* Convert your CSVs into binary hdf5 files with hdfio.
* Use `pbdMPI::get.jid()` to determine which rows each process will read in.
* Read in parallel with `hdfio::read_h5df()` by setting the `rows` argument to the return from `get.jid()`.
* Use the pbdDMAT or kazaam packages to compute on the distributed data.
